{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the data,in this notebook we are using the Qitta data from the paper \"Machine learningâ€“based feature selection to search stable microbial biomarkers: application to inflammatory bowel disease\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 datasets, in this work they will be combined together at species and genus level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 3 datasets comes from :\n",
    "1) 96 sampels: Lloyd-Price J, Arze C, Ananthakrishnan AN, et al. Multi-omics of the gut microbial ecosystem in inflammatory bowel diseases. Nature 2019\n",
    "2) 836 samples: Flores GE, Caporaso JG, Henley JB, et al. Temporal variability is a personalized feature of the human microbiome. Genome Biol 2014\n",
    "3) 637 samples: Halfvarson J, Brislawn CJ, Lamendella R, et al. Dynamics of the human gut microbiome in inflammatory bowel disease. Nat Microbiol 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m../../../Code\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mloadData\u001b[39;00m \n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRunML\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFS\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmetric\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lustre/isaac24/scratch/mhe8/SelectMicro_24/Analysis/Qitta/script/../../../Code/RunML.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier  \u001b[38;5;66;03m# Import XGBoost\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#from sklearn import svm, datasets\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auc, roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, RocCurveDisplay, confusion_matrix, ConfusionMatrixDisplay\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../../Code')\n",
    "import loadData \n",
    "import RunML\n",
    "import FS\n",
    "import metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features_species = np.load('../data/total_features_species.npy')\n",
    "total_features_genus = np.load('../data/total_features_genus.npy')\n",
    "total_label= np.load('../data/total_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(total_features_species.shape)\n",
    "print(total_features_genus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_features_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y= le.fit_transform(total_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y).value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculating H score for each OTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxlabels = ['Species','Genus']\n",
    "feature_df_list = [pd.DataFrame(total_features_species,\n",
    "                                         columns = [f\"column_{i+1}\" for i in range(total_features_species.shape[1])]),\n",
    "                   pd.DataFrame(total_features_genus,\n",
    "                                         columns = [f\"column_{i+1}\" for i in range(total_features_genus.shape[1])])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedresult_list = []\n",
    "for feature_df in feature_df_list:\n",
    "    selectedresult=FS.SelectMicro_fun(feature_df,y)\n",
    "    selectedresult_list.append(selectedresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "### Prepare other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurenames_full = [feature_df_list[0].columns,feature_df_list[1].columns]\n",
    "selectedOTU_index_Lasso_list = []\n",
    "selectedOTU_index_FS_lasso_list = []\n",
    "selectedOTU_index_Lasso_FS_list = []\n",
    "\n",
    "data_subset_list = []\n",
    "\n",
    "for selectedresult in selectedresult_list:\n",
    "    selectedOTU_index_FS = selectedresult['selected_indices']\n",
    "\n",
    "    data = selectedresult['relative_abundance_data']\n",
    "    X_FS = selectedresult['selected_data']\n",
    "\n",
    "    X_lasso_ft,selectedOTU_index_Lasso  = RunML.LassoFS_CV(data,y)\n",
    "    selectedOTU_index_Lasso_list.append(selectedOTU_index_Lasso)\n",
    "\n",
    "    X_FS_lasso_ft,xlabel_FS_lasso_ft0  = RunML.LassoFS_CV(X_FS,y)\n",
    "    selectedOTU_index_FS_lasso = selectedOTU_index_FS[xlabel_FS_lasso_ft0]\n",
    "    selectedOTU_index_FS_lasso_list.append(selectedOTU_index_FS_lasso)\n",
    "    \n",
    "    selectedOTU_index_Lasso_FS = np.intersect1d(selectedOTU_index_Lasso, selectedOTU_index_FS)\n",
    "    selectedOTU_index_Lasso_FS_list.append(selectedOTU_index_Lasso_FS)\n",
    "    X_lasso_FS = data[:,selectedOTU_index_Lasso_FS]\n",
    "    \n",
    "    \n",
    "    data_subset = {\"AllFeatures\":data,\n",
    "               \"SelectMicro\": X_FS,\n",
    "               \"Lasso_finetune\":X_lasso_ft,\n",
    "               \"FS_Lassofinetune\":X_FS_lasso_ft,\n",
    "               \"Lassofinetune_FS\":X_lasso_FS\n",
    "              }\n",
    "    data_subset_list.append(data_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_subset_list:\n",
    "    print(f'The shape of the original dataset is ',np.shape(data['AllFeatures']))\n",
    "    print(f'The shape of the SelectMicro dataset is ',np.shape(data['SelectMicro']))\n",
    "    print(f'The shape of the Lasso_finetune selected dataset is ',np.shape(data['Lasso_finetune']))\n",
    "    print(f'The shape of the FS_Lasso_finetune selected dataset is ',np.shape(data['FS_Lassofinetune']))\n",
    "    print(f'The shape of the Lasso_finetune_FS selected dataset is ',np.shape(data['Lassofinetune_FS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = [\"RF\",\"SVM\", \"CatBoost\",\"NB\",\"xgboost\"]\n",
    "for i , dataset  in enumerate(data_subset_list):\n",
    "    print(f\"Analysis for {taxlabels[i]}\")\n",
    "    dict_cm = RunML.runClassifier_FScompare(data_subsets= dataset,y= y,classifiers=cls)\n",
    "    print(metric.metric_sum(dict_cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchRF(X_df,y,SMOTE=False,k=5):\n",
    "\n",
    "   \n",
    "    df = X_df\n",
    "    \n",
    "    ft_list = X.columns\n",
    "\n",
    "    X = df.to_numpy()\n",
    "\n",
    "    # configure the cross-validation procedure\n",
    "    # Set up 5-fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=777)\n",
    "\n",
    "    # performance reports\n",
    "    accuracy_results = list()\n",
    "    f1_results = list()\n",
    "    precision_results = list()\n",
    "    recall_results = list()\n",
    "\n",
    "    # preparation for ROC curve\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    enriched_all = pd.DataFrame(ft_list, columns = ['Taxa'])\n",
    "    list_shap_values = list()\n",
    "    list_test_sets = list()\n",
    "\n",
    "    \n",
    "    idx = 0\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        \n",
    "    \n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "\n",
    "\n",
    "        # configure the cross-validation procedure\n",
    "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "        # define the model\n",
    "        clf = RandomForestClassifier(n_jobs=5, random_state=777)\n",
    "\n",
    "        # define search space\n",
    "        space = dict()\n",
    "        space['n_estimators'] = [100, 200, 500, 750, 1000, 1500, 2000]\n",
    "        space['max_depth'] = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "        space['min_samples_leaf'] = [1, 2, 4]\n",
    "        space['min_samples_split'] = [2, 5, 10]\n",
    "        space['max_features'] = ['sqrt', 'log2']\n",
    "\n",
    "        # define search\n",
    "        search = GridSearchCV(clf, space, scoring='accuracy', n_jobs=-1, cv=cv_inner, refit=True)\n",
    "        # execute search\n",
    "        result = search.fit(X_train, y_train)\n",
    "\n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_model = result.best_estimator_\n",
    "        \n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = best_model.predict(X_test)\n",
    "        # evaluate the model\n",
    "        acc = accuracy_score(y_test, yhat)\n",
    "        f1 = f1_score(y_test, yhat)\n",
    "        prec = precision_score(y_test, yhat)\n",
    "        rec = recall_score(y_test, yhat)\n",
    "\n",
    "        # store the result\n",
    "        accuracy_results.append(acc)\n",
    "        f1_results.append(f1)\n",
    "        precision_results.append(prec)\n",
    "        recall_results.append(rec)\n",
    "\n",
    "        # ROC curve\n",
    "        best_model.fit(X_train, y_train)\n",
    "        viz = RocCurveDisplay.from_estimator(\n",
    "            best_model,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            name=f\"Best model {idx+1}\",\n",
    "            alpha=0.3,\n",
    "            lw=1,\n",
    "            ax=ax,\n",
    "        )\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)# interpolate TPR values at mean FPR points based on output\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)#AUC value\n",
    "\n",
    "        # SHAP values\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "        shap_obj = explainer(X_test)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        list_shap_values.append(shap_values)\n",
    "        list_test_sets.append(test_ix)\n",
    "        \n",
    "\n",
    "        high_index = pd.DataFrame(shap_obj.data, columns=shap_obj.feature_names, index=X_test.index).idxmax()# finds the feature with the highest value for each sample.\n",
    "        shap_1 = pd.DataFrame(shap_values[1], columns=shap_obj.feature_names, index=X_test.index)\n",
    "        \n",
    "        enriched = list()\n",
    "        for v, i in high_index.items():\n",
    "            sv = shap_1[v].loc[i]\n",
    "            if sv<0:\n",
    "                sv = \"Level 0\"\n",
    "            else:\n",
    "                sv = \"Level 1\"\n",
    "            enriched.append(\n",
    "                {\n",
    "                    'Taxa': v,\n",
    "                    'enriched': sv\n",
    "                }\n",
    "            )\n",
    "        enriched = pd.DataFrame(enriched)\n",
    "        enriched.rename(columns={'enriched': 'enriched{}'.format(idx+1)}, inplace=True)\n",
    "        enriched_all = enriched_all.merge(enriched, on='Taxa', how='outer')\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    # continue ROC\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(\n",
    "        mean_fpr,\n",
    "        mean_tpr,\n",
    "        color=\"b\",\n",
    "        label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(\n",
    "        mean_fpr,\n",
    "        tprs_lower,\n",
    "        tprs_upper,\n",
    "        color=\"grey\",\n",
    "        alpha=0.2,\n",
    "        label=r\"$\\pm$ 1 std. dev.\",\n",
    "    )\n",
    "\n",
    "    ax.set(\n",
    "        xlim=[-0.05, 1.05],\n",
    "        ylim=[-0.05, 1.05],\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"True Positive Rate\",\n",
    "        title=f\"Mean ROC curve on {rank} rank\",\n",
    "    )\n",
    "    ax.axis(\"square\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    #plt.show()\n",
    "\n",
    "    #combining results from all iterations\n",
    "    test_set = list_test_sets[0]\n",
    "    shap_values = np.array(list_shap_values[0])\n",
    "    for i in range(1,len(list_test_sets)):\n",
    "        test_set = np.concatenate((test_set,list_test_sets[i]),axis=0)\n",
    "        shap_values = np.concatenate((shap_values,np.array(list_shap_values[i])),axis=1)\n",
    "\n",
    "    #bringing back variable names    \n",
    "    X_test = pd.DataFrame(X.iloc[test_set])\n",
    "    X_test.columns = ft_list\n",
    "    \n",
    "    #creating explanation plot for the whole experiment\n",
    "    #shap.summary_plot(shap_values[1], X_test)\n",
    "    enriched_all.to_csv('../data/SHAP/SHAP_enriched_' + rank + '.csv', index=False)\n",
    "\n",
    "    # SHAP feature importances\n",
    "    mean_shap_feature_values = pd.DataFrame(shap_values[1], columns=ft_list).abs().mean(axis=0).sort_values(ascending=False)\n",
    "    mean_shap_feature_values.index.name = 'features'\n",
    "    mean_shap_feature_values.name = 'mean_shap'\n",
    "    mean_shap_feature_values = mean_shap_feature_values.reset_index()\n",
    "    mean_shap_feature_values.to_csv('data/SHAP_feature_importance_' + rank + '.csv', index=False)\n",
    "\n",
    "    # summarize the estimated performance of the model\n",
    "    print('Accuracy: %.3f (%.3f), F1: %.3f (%.3f), Precision: %.3f (%.3f), Recall: %.3f (%.3f)' % (np.mean(accuracy_results), np.std(accuracy_results), np.mean(f1_results), np.std(f1_results), np.mean(precision_results), np.std(precision_results), np.mean(recall_results), np.std(recall_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i , datasubset  in enumerate(data_subset_list):#[['Species','Genus']]\n",
    "   \n",
    "    for index, (key, value) in enumerate(datasubset.items()):# 5 different feature selection method\n",
    "        print(f\"Run RF model for: rank:{taxlabels[i]}, feature selection method: {key}\")\n",
    "        dict_cm = RunML.runClassifier_FScompare(data_subsets= dataset,y= y,classifiers=cls)\n",
    "        print(metric.metric_sum(dict_cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary of results in latex\n",
    "# in genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dim_reduction(X, y, method='PCA', perplexity=30, n_components=2,datalabel=None):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Apply dimensionality reduction\n",
    "    if method.upper() == 'PCA':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X_scaled)\n",
    "        title = f'PCA Plot {datalabel}'\n",
    "    elif method.upper() == 'TSNE':\n",
    "        reducer = TSNE(n_components=n_components, perplexity=perplexity, random_state=42)\n",
    "        X_reduced = reducer.fit_transform(X_scaled)\n",
    "        title = f't-SNE Plot {datalabel}'\n",
    "    else:\n",
    "        raise ValueError(\"Method should be either 'PCA' or 'tSNE'\")\n",
    "\n",
    "    # Create DataFrame for plotting\n",
    "    df_plot = pd.DataFrame(X_reduced, columns=[f'Component {i+1}' for i in range(n_components)])\n",
    "    df_plot['Label'] = y\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    for label in np.unique(y):\n",
    "        subset = df_plot[df_plot['Label'] == label]\n",
    "        plt.scatter(subset.iloc[:, 0], subset.iloc[:, 1], label=f'Class {label}', alpha=0.7)\n",
    "\n",
    "    plt.xlabel(df_plot.columns[0])\n",
    "    plt.ylabel(df_plot.columns[1])\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# plot the family \n",
    "for datatype, subset in data_subset_list[0].items():   \n",
    "    RunML.plot_dim_reduction(subset, y, method='PCA',datalabel=datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the genus \n",
    "for datatype, subset in data_subset_list[1].items():    \n",
    "     RunML.plot_dim_reduction(subset, y, method='PCA',datalabel=datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare the first 15 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# the   df with the largest H statistics features\n",
    "entries=15\n",
    "selectedOTU_index_15=selectedOTU_index[:entries]\n",
    "X_FS_15=data[:,selectedOTU_index_15]\n",
    "df=pd.DataFrame(data=X_FS_15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the column names of the featues\n",
    "ASVs = cols_name\n",
    "selectedASVs=[ASVs[i] for i in selectedOTU_index_15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(targetLabel))\n",
    "RunML.plotPresenseRatio(X_FS_15,targetLabel,selectedASVs,posLabel=\"IBD\",posText=\"IBD\",negText=\"nonIBD\",entries=entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedASV_lasso = [cols_name[i] for i in xlabel_lasso]\n",
    "RunML.plotPresenseRatio(X_lasso,targetLabel,selectedASV_lasso,posLabel=\"IBD\",posText=\"IBD\",negText=\"nonIBD\",entries=len(selectedASV_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedASVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qitta_combine[['Diagnosis','X4414821']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedASV_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
